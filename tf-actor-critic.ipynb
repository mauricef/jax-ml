{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tT4N3qYviUJr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as onp\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Set seed for experiment reproducibility\n",
    "seed = 50\n",
    "env.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "onp.random.seed(seed)\n",
    "\n",
    "# Small epsilon value for stabilizing division operations\n",
    "eps = onp.finfo(onp.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aXKbbMC-kmuv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_actions, \n",
    "        num_hidden_units):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.common = layers.Dense(num_hidden_units, activation=\"relu\")\n",
    "        self.actor = layers.Dense(num_actions)\n",
    "        self.critic = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nWyxJgjLn68c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-13 12:11:17.372085: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "num_actions = env.action_space.n  # 2\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5URrbGlDSAGx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def env_step(action):\n",
    "\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (state.astype(onp.float32), \n",
    "          onp.array(reward, onp.int32), \n",
    "          onp.array(done, onp.int32))\n",
    "\n",
    "def tf_env_step(action):\n",
    "    return tf.numpy_function(env_step, [action], \n",
    "                           [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "a4qVRV063Cl9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_episode(initial_state, model, max_steps):\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        action_logits_t, value = model(state)\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jpEwFyl315dl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_expected_return(rewards, gamma, standardize=True):\n",
    "    n = tf.shape(rewards)[0]\n",
    "    returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        returns = returns.write(i, discounted_sum)\n",
    "    returns = returns.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "                   (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9EXwbEez6n9m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs, values, returns): \n",
    "    advantage = returns - values\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
    "    critic_loss = huber_loss(values, returns)\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QoccrkF3IFCg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(initial_state, model, optimizer, gamma, max_steps_per_episode):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs, values, rewards = run_episode(\n",
    "            initial_state, model, max_steps_per_episode) \n",
    "\n",
    "        returns = get_expected_return(rewards, gamma)\n",
    "        action_probs, values, returns = [\n",
    "            tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "        loss = compute_loss(action_probs, values, returns)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kbmBxnzLiUJx",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]2021-09-13 12:11:18.540590: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "Episode 482:   5%| | 482/10000 [00:19<06:21, 24.96it/s, episode_reward=200, runn"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solved at episode 482: average reward: 195.12!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 1000\n",
    "reward_threshold = 195\n",
    "running_reward = 0\n",
    "gamma = 0.99\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    for i in t:\n",
    "        initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "        episode_reward = int(train_step(\n",
    "            initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
    "\n",
    "        episodes_reward.append(episode_reward)\n",
    "        running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "        t.set_description(f'Episode {i}')\n",
    "        t.set_postfix(\n",
    "            episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "        if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "            break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_jQ1tEQCxwRx"
   ],
   "name": "actor_critic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
